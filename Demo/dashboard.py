import streamlit as st
import requests
import time
import pandas as pd
from datetime import datetime

# === é…ç½® ===
API_URL = "http://192.168.1.18:8000"
REFRESH_RATE = 1.0  # åˆ·æ–°é¢‘ç‡(ç§’)

st.set_page_config(
    page_title="Elysia Monitor",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# === CSS ç¾åŒ– ===
st.markdown("""
<style>
    /* å…¨å±€å­—ä½“ä¼˜åŒ– */
    .block-container { padding-top: 1rem; padding-bottom: 1rem; }
    
    /* å¡ç‰‡æ ·å¼ */
    .metric-card {
        background-color: #f8f9fa;
        border: 1px solid #e9ecef;
        padding: 15px;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        margin-bottom: 10px;
    }
    
    /* çŠ¶æ€æŒ‡ç¤ºç¯ */
    .status-indicator {
        display: inline-block;
        width: 10px;
        height: 10px;
        border-radius: 50%;
        margin-right: 5px;
    }
    .status-on { background-color: #28a745; box-shadow: 0 0 5px #28a745; }
    .status-off { background-color: #dc3545; }
    
    /* æ€ç»´é“¾æ°”æ³¡ */
    .thought-bubble {
        background-color: #e3f2fd;
        border-left: 4px solid #2196f3;
        padding: 10px 15px;
        border-radius: 0 10px 10px 0;
        margin-bottom: 10px;
        font-style: italic;
        color: #0d47a1;
    }
    .reply-bubble {
        background-color: #ffffff;
        border: 1px solid #ddd;
        padding: 10px 15px;
        border-radius: 10px;
        color: #333;
    }
    
    /* è®°å¿†æ—¥å¿—è¡¨æ ¼ä¼˜åŒ– */
    .dataframe { font-size: 0.8rem !important; }
</style>
""", unsafe_allow_html=True)

def fetch_state():
    """ä» FastAPI è·å–å…¨é‡çŠ¶æ€"""
    try:
        resp = requests.get(f"{API_URL}/dashboard/snapshot", timeout=0.5)
        if resp.status_code == 200:
            return resp.json()
    except Exception:
        return None
    return None

def format_timestamp(ts):
    """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
    if isinstance(ts, (int, float)) and ts > 0:
        return datetime.fromtimestamp(ts).strftime("%H:%M:%S")
    return "Never"

# === ä¸»ç•Œé¢ ===
st.title("ğŸ§  Elysia Neural Dashboard")

# å ä½å®¹å™¨ï¼Œç”¨äºè‡ªåŠ¨åˆ·æ–°
main_container = st.empty()

while True:
    state = fetch_state()
    
    with main_container.container():
        if not state:
            st.error("âš ï¸ Connection Lost. Waiting for Elysia Server...")
            time.sleep(2)
            continue

        # ==========================================
        # 1. é¡¶æ ï¼šç³»ç»Ÿå¥åº·åº¦ & æ ¸å¿ƒæŒ‡æ ‡ (System & L3)
        # ==========================================
        sys = state.get("system", {})
        l3 = state.get("l3_persona", {})
        l0 = state.get("l0_sensor", {})
        actuator = state.get("actuator", {})
        
        # ä½¿ç”¨ HTML è‡ªå®šä¹‰çŠ¶æ€æ 
        dispatcher_status = "status-on" if sys.get("dispatcher_alive") else "status-off"
        
        cols = st.columns([1, 1, 1, 1, 2])
        
        with cols[0]:
            st.markdown(f'<div class="metric-card"><b>Dispatcher</b><br><span class="status-indicator {dispatcher_status}"></span>{"Alive" if sys.get("dispatcher_alive") else "Dead"}</div>', unsafe_allow_html=True)
            
        with cols[1]:
            st.markdown(f'<div class="metric-card"><b>Online Clients</b><br>ğŸ”Œ {sys.get("online_clients", 0)}</div>', unsafe_allow_html=True)
            
        with cols[2]:
            st.markdown(f'<div class="metric-card"><b>Input Queue</b><br>ğŸ“¥ {l0.get("input_queue_size", 0)}</div>', unsafe_allow_html=True)

        with cols[3]:
            # Mood æ˜¾ç¤º
            mood = l3.get("mood", "Neutral")
            mood_color = "orange" if mood in ["Sad", "Angry"] else "green"
            st.markdown(f'<div class="metric-card"><b>Current Mood</b><br><span style="color:{mood_color}; font-weight:bold">{mood}</span></div>', unsafe_allow_html=True)

        with cols[4]:
            # Actuator Channels
            channels = actuator.get("registered_channels", [])
            st.markdown(f'<div class="metric-card"><b>Actuator Channels</b><br>ğŸ“¢ {", ".join(channels)}</div>', unsafe_allow_html=True)

        # ==========================================
        # 2. æ ¸å¿ƒåŠŸèƒ½åŒº (Tabs)
        # ==========================================
        tab_brain, tab_memory, tab_reflector, tab_raw = st.tabs(["ğŸ§  Brain & Thought", "ğŸ’¾ Memory & Session", "ğŸª Reflector Logs", "ğŸ“ Raw Data"])
        
        # --- Tab 1: Brain (L1) ---
        with tab_brain:
            l1 = state.get("l1_brain", {})
            col_b1, col_b2 = st.columns([1, 2])
            
            with col_b1:
                st.info("Configuration")
                st.text(f"Model: {l1.get('model_name')}")
                st.text(f"Temp:  {l1.get('temperature')}")
                
                # æ˜¾ç¤º System Prompt (æŠ˜å )
                with st.expander("Show L3 System Prompt"):
                    st.code(l3.get("prompt", ""), language="text")

            with col_b2:
                st.subheader("Last Thinking Process")
                log = l1.get("last_thinking_log")
                
                if log:
                    # åŒºåˆ†ä¸»åŠ¨å›å¤ (Active) å’Œ è¢«åŠ¨å›å¤ (Normal)
                    is_active = "should_speak" in log
                    
                    if is_active:
                        st.caption(f"Type: Active Decision | Should Speak: {log.get('should_speak')}")
                        inner = log.get("inner_voice", "")
                    else:
                        st.caption("Type: Response Generation")
                        inner = log.get("inner_thought", "")
                    
                    # æ¸²æŸ“æ€ç»´æ°”æ³¡
                    if inner:
                        st.markdown(f"""
                        <div class="thought-bubble">
                            <b>ğŸ’­ Inner Thought:</b><br>{inner}
                        </div>
                        """, unsafe_allow_html=True)
                    
                    # æ¸²æŸ“å›å¤æ°”æ³¡
                    reply = log.get("public_reply")
                    if reply:
                        st.markdown(f"""
                        <div class="reply-bubble">
                            <b>ğŸ—£ï¸ Elysia:</b> {reply}
                        </div>
                        """, unsafe_allow_html=True)
                    else:
                        st.markdown("*No public reply generated.*")
                else:
                    st.warning("No thoughts recorded yet.")

        # --- Tab 2: Memory (L2) ---
        with tab_memory:
            l2 = state.get("l2_memory", {})
            sess = l2.get("session_status", {})
            
            # é¡¶éƒ¨æŒ‡æ ‡
            m1, m2, m3, m4 = st.columns(4)
            m1.metric("Session Role", sess.get("role", "AI"))
            m2.metric("User Name", sess.get("user_name", "User"))
            
            # æ¶ˆæ¯çª—å£è¿›åº¦æ¡
            curr = sess.get("total_messages", 0)
            limit = sess.get("max_messages_limit", 20)
            # é˜²æ­¢é™¤ä»¥0é”™è¯¯
            progress = min(curr / limit, 1.0) if limit > 0 else 0
            m3.metric("Context Window", f"{curr} / {limit}")
            m3.progress(progress)
            
            m4.metric("Last Interaction", format_timestamp(sess.get("last_interaction_time", 0)))
            
            st.divider()
            
            # === æ–°å¢ï¼šèŠå¤©è®°å½•å¯è§†åŒ– ===
            st.subheader("ğŸ’¬ Recent Conversation (Context Window)")
            
            # è·å–ä½ æ–°åŠ çš„å­—æ®µ
            recent_msgs = sess.get("last_few_messages", [])
            
            if recent_msgs:
                # åˆ›å»ºä¸€ä¸ªèŠå¤©å®¹å™¨
                chat_container = st.container(height=400) # å›ºå®šé«˜åº¦ï¼Œå¯æ»šåŠ¨
                with chat_container:
                    for msg in recent_msgs:
                        role = msg.get("role", "user")
                        content = msg.get("content", "")
                        
                        # æ˜ å°„å¤´åƒ
                        if role == "å¦–æ¢¦":
                            avatar = "ğŸ‘¤"
                            # ä¹Ÿå¯ä»¥æ ¹æ®ä½ çš„ UserMessage ç»“æ„æ˜¾ç¤ºæ—¶é—´æˆ³
                            # ts = format_timestamp(msg.get("client_timestamp", 0))
                        else:
                            avatar = "ğŸ¤–" # æˆ–è€…ç”¨ä½ çš„ Elysia å¤´åƒ URL
                            
                        # ä½¿ç”¨ Streamlit åŸç”ŸèŠå¤©ç»„ä»¶
                        with st.chat_message(name=role, avatar=avatar):
                            st.markdown(content)
            else:
                st.info("No conversation history yet.")

            st.divider()
            
            # åº•éƒ¨æ˜¾ç¤ºå‘é‡åº“ä¿¡æ¯
            st.caption(f"ğŸ“š Vector DB: Micro='{l2.get('micro_memory_collection')}' | Macro='{l2.get('macro_memory_collection')}'")
            
        # --- Tab 3: Reflector ---
        with tab_reflector:
            ref = state.get("reflector", {})
            
            # ==================================================
            # 1. æš´åŠ›å¯»è·¯ (Robust Data Finder)
            # ==================================================
            # å®šä¹‰ä¸€ä¸ªå†…éƒ¨å‡½æ•°ï¼Œä¸“é—¨ç”¨æ¥åœ¨å­—å…¸é‡Œæ‰¾ "last_macro_reflection_log"
            def find_key_in_dict(source_dict, target_key):
                # A. ç›´æ¥åœ¨å½“å‰å±‚æ‰¾
                if target_key in source_dict:
                    return source_dict[target_key]
                # B. åœ¨å­å­—å…¸é‡Œæ‰¾ (æ¯”å¦‚ macro_reflector_status)
                for key, value in source_dict.items():
                    if isinstance(value, dict) and target_key in value:
                        return value[target_key]
                return None

            # æŸ¥æ‰¾ Macro æ•°æ®
            macro_logs = find_key_in_dict(ref, "last_macro_reflection_log") or []
            macro_time = find_key_in_dict(ref, "last_macro_reflection_time") or "Never"
            
            # æŸ¥æ‰¾ Micro æ•°æ®
            micro_logs = find_key_in_dict(ref, "last_micro_reflection_log") or []
            micro_time = find_key_in_dict(ref, "last_micro_reflection_time") or "Never"
            
            # ç¼“å†²æ± å¤§å° (é€šå¸¸ç›´æ¥åœ¨ ref å±‚)
            buffer_size = ref.get("buffer_size", 0)

            # ==================================================
            # 2. æ¸²æŸ“ç•Œé¢
            # ==================================================
            
            # æ¦‚è§ˆæŒ‡æ ‡
            r1, r2, r3 = st.columns(3)
            r1.metric("Buffer Size", buffer_size)
            r2.metric("Last Micro Run", str(micro_time))
            r3.metric("Last Macro Run", str(macro_time))
            
            st.divider()
            
            col_micro, col_macro = st.columns([1, 1.3])
            
            # --- Micro Logs ---
            with col_micro:
                st.subheader(f"Micro-Reflections ({len(micro_logs)})")
                if micro_logs and isinstance(micro_logs, list):
                    # æ•°æ®æ¸…æ´—ï¼šè½¬å­—ç¬¦ä¸²é˜²æ­¢æ¸²æŸ“é”™è¯¯
                    clean_micro = [{k: str(v) for k, v in item.items()} for item in micro_logs]
                    st.dataframe(clean_micro, use_container_width=True, hide_index=True)
                else:
                    st.info("No micro-memories yet.")

            # --- Macro Logs (ä¿®å¤é‡ç‚¹) ---
            with col_macro:
                st.subheader(f"Macro-Reflections ({len(macro_logs)})")
                
                if macro_logs and isinstance(macro_logs, list):
                    for i, log in enumerate(macro_logs):
                        # æå–æ•°æ®
                        ts = log.get("timestamp", 0)
                        # å¦‚æœ timestamp æ˜¯ float/intï¼Œå°è¯•æ ¼å¼åŒ–ï¼Œå¦åˆ™ç›´æ¥æ˜¾ç¤º
                        if isinstance(ts, (int, float)):
                            time_str = datetime.fromtimestamp(ts).strftime("%H:%M:%S")
                        else:
                            time_str = str(ts)
                            
                        emotion = log.get("dominant_emotion", "Unknown")
                        poignancy = log.get("poignancy", "?")
                        subject = log.get("subject", "General")
                        
                        label = f"ğŸ“… {time_str} | {subject} - {emotion} (Intensity: {poignancy})"
                        
                        with st.expander(label, expanded=(i==0)):
                            # æ ‡ç­¾æ¸²æŸ“
                            tags = log.get("keywords", [])
                            if isinstance(tags, list):
                                st.markdown(" ".join([f"`#{t}`" for t in tags]))
                            
                            st.divider()
                            
                            # å†…å®¹æ¸²æŸ“
                            content = log.get("diary_content", "No content")
                            st.caption("Diary Content:")
                            st.markdown(f"> {content}")
                            
                            # åŸå§‹æ•°æ®è°ƒè¯• (å¯é€‰)
                            # st.json(log)
                else:
                    st.info("No macro-memories yet.")
                    
                    # === ç»ˆæè°ƒè¯•é¢æ¿ ===
                    # å¦‚æœè¿˜æ˜¯æ˜¾ç¤ºä¸å‡ºæ¥ï¼Œå±•å¼€è¿™ä¸ªçœ‹çœ‹åˆ°åº• ref é•¿ä»€ä¹ˆæ ·
                    with st.expander("ğŸ•µï¸ Debug: Inspect Raw Reflector State", expanded=False):
                        st.write("Streamlit sees this data structure for 'reflector':")
                        st.json(ref)

        # --- Tab 4: Raw Data ---
        with tab_raw:
            st.json(state)

    time.sleep(REFRESH_RATE)